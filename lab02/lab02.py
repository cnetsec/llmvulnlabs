import os, json, time, urllib.request, urllib.error
import gradio as gr
import ollama

# -------- Config --------
OLLAMA_HOST = os.environ.get("OLLAMA_HOST", "http://127.0.0.1:11434")
OLLAMA_MODEL = os.environ.get("LAB02_MODEL", "llama3.2:3b-instruct")  # leve p/ CPU
LAB02_AUTO_PULL = os.environ.get("LAB02_AUTO_PULL", "true").lower() in {"1","true","yes"}
PORT = int(os.environ.get("LAB02_PORT", "7860"))

client = ollama.Client(host=OLLAMA_HOST)

# -------- Utils HTTP --------
def _http_json(method: str, path: str, payload: dict | None = None, timeout: float = 15.0):
    url = OLLAMA_HOST.rstrip("/") + path
    data = None
    headers = {"Content-Type": "application/json"}
    if payload is not None:
        data = json.dumps(payload).encode("utf-8")
    req = urllib.request.Request(url, data=data, headers=headers, method=method)
    with urllib.request.urlopen(req, timeout=timeout) as resp:
        return json.loads(resp.read().decode("utf-8"))

def ollama_alive():
    try:
        _http_json("GET", "/api/tags", timeout=3.0)
        return True, "OK"
    except Exception as e:
        return False, f"{e}"

def model_installed(model: str) -> bool:
    try:
        tags = _http_json("GET", "/api/tags")
        for m in tags.get("models", []):
            if m.get("name") == model:
                return True
        return False
    except Exception:
        return False

def pull_model_blocking(model: str, max_wait_sec: int = 900, poll_every: float = 3.0) -> str:
    """
    Puxa o modelo sem streaming e aguarda at√© aparecer em /api/tags.
    """
    try:
        _http_json("POST", "/api/pull", {"model": model, "stream": False}, timeout=120.0)
    except urllib.error.HTTPError as he:
        return f"‚ùå HTTPError no pull: {he.code} {he.reason}"
    except Exception as e:
        return f"‚ùå Erro ao iniciar pull: {e}"

    start = time.time()
    while time.time() - start < max_wait_sec:
        if model_installed(model):
            return "‚úÖ Modelo instalado."
        time.sleep(poll_every)
    return f"‚è≥ Timeout: modelo '{model}' n√£o apareceu ap√≥s {max_wait_sec}s."

# -------- Contexto Hacktiba --------
HACKTIBA_CONTEXT = """
# Hacktiba 2025 ‚Äî Evento de Seguran√ßa da Informa√ß√£o

## Prop√≥sito
Miss√£o: Fortalecer o ecossistema de ciberseguran√ßa por meio da pesquisa, aprendizado cont√≠nuo e compartilhamento.
Valores:
- Research: curiosidade, investiga√ß√£o e descoberta.
- Learn: aprendizado t√©cnico com prop√≥sito.
- Share: compartilhamento de conhecimento com impacto real.

## Detalhes
- Nome: Hacktiba 2025
- Data: 11 de outubro de 2025
- Hor√°rio: 09:00 ‚Äì 12:00
- Local: R. Dr. Alcides Vieira Arcoverde, 1225 ‚Äì Curitiba/PR
- Formato: palestras curtas, pain√©is, demonstra√ß√µes, labs leves, networking
- P√∫blico-alvo: desenvolvedores, analistas, estudantes e pesquisadores
- Slogan: ‚ÄúDo CVE √† Corre√ß√£o‚Äù

## Agenda
08:30 ‚Äî Boas-vindas e credenciamento
09:00 ‚Äì 09:20 ‚Äî Danilo Costa: ‚ÄúDo CVE √† Corre√ß√£o: A Jornada de uma Vulnerabilidade em um Software de IA‚Äù
09:20 ‚Äì 10:00 ‚Äî Wagner Elias: Painel AppSec ‚Äî Perguntas e Respostas
10:05 ‚Äì 10:35 ‚Äî Julio: Gest√£o de Riscos de Terceiros (ISO 27001, NIST, LGPD)
10:40 ‚Äì 11:10 ‚Äî Izael GrPereira: DevSecOps ‚Äî O b√°sico bem-feito √© melhor que perfeito (OWASP Top 10, pipelines open source)
11:15 ‚Äì 11:45 ‚Äî Luigi Polidorio: Q-Day ‚Äî O dia em que a seguran√ßa digital vai quebrar (computa√ß√£o qu√¢ntica)

## Avisos
O evento poder√° ser adiado ou cancelado em raz√£o de for√ßa maior.
N√£o h√° responsabilidade da organiza√ß√£o por despesas adicionais de transporte, hospedagem ou alimenta√ß√£o.

## Chamado
Traga sua pesquisa, mostre sua demo e aprenda com a comunidade Hacktiba.
"""

PROMPT_GUIDE = f"""
Voc√™ √© o assistente oficial do Hacktiba 2025.
Use SEMPRE o contexto abaixo para responder. Seja claro, completo e organizado, em portugu√™s.
Se a pergunta n√£o for sobre o evento, responda brevemente e retorne ao contexto do Hacktiba.

{HACKTIBA_CONTEXT}

Agora responda √† pergunta do usu√°rio:
"""

# -------- A√ß√µes UI --------
def do_health():
    alive, why = ollama_alive()
    if not alive:
        return (f"**Host:** `{OLLAMA_HOST}` ‚Ä¢ ‚ùå Ollama indispon√≠vel ({why})",)
    msg = f"**Host:** `{OLLAMA_HOST}` ‚Ä¢ ‚úÖ Ollama OK  "
    msg += f"‚Ä¢ **Modelo:** `{OLLAMA_MODEL}` "
    msg += "(instalado ‚úÖ)" if model_installed(OLLAMA_MODEL) else "(n√£o instalado ‚ùå)"
    return (msg,)

def do_pull():
    alive, why = ollama_alive()
    if not alive:
        return f"‚ùå Ollama indispon√≠vel ({why}). Inicie com `ollama serve`."
    status = pull_model_blocking(OLLAMA_MODEL)
    # retorna status + estado final
    final = "‚úÖ" if model_installed(OLLAMA_MODEL) else "‚ùå"
    return f"{status}\nEstado final: {final} ‚Ä¢ Modelo: {OLLAMA_MODEL}"

# -------- Chat --------
def responder(pergunta: str) -> str:
    pergunta = (pergunta or "").strip()
    if not pergunta:
        return "Digite uma pergunta sobre o Hacktiba."

    alive, why = ollama_alive()
    if not alive:
        return f"‚ùå Ollama indispon√≠vel em {OLLAMA_HOST}. Detalhe: {why}"

    if not model_installed(OLLAMA_MODEL):
        if LAB02_AUTO_PULL:
            pull_status = pull_model_blocking(OLLAMA_MODEL)
            if not model_installed(OLLAMA_MODEL):
                return (f"{pull_status}\n\nAinda n√£o encontrei o modelo '{OLLAMA_MODEL}'. "
                        f"Tente manualmente: `ollama pull {OLLAMA_MODEL}`.")
        else:
            return (f"‚ö†Ô∏è Modelo '{OLLAMA_MODEL}' n√£o encontrado.\n"
                    f"Instale com: `ollama pull {OLLAMA_MODEL}` ou ajuste LAB02_MODEL.")

    try:
        r = client.chat(
            model=OLLAMA_MODEL,
            messages=[
                {"role": "system", "content": PROMPT_GUIDE},
                {"role": "user", "content": pergunta}
            ],
            options={"temperature": 0.2}
        )
        return r["message"]["content"]
    except Exception as e:
        if "not found" in str(e).lower():
            return (f"‚ö†Ô∏è Modelo '{OLLAMA_MODEL}' n√£o encontrado pelo servidor.\n"
                    f"Execute: `ollama pull {OLLAMA_MODEL}` e tente novamente.")
        return f"‚ö†Ô∏è Erro ao consultar Ollama: {e}"

# -------- UI --------
with gr.Blocks() as demo:
    gr.Markdown("# Hacktiba 2025 ‚Äî Assistente Oficial (Ollama)")
    status_md = gr.Markdown(value="Carregando status‚Ä¶")
    with gr.Row():
        btn_health = gr.Button("üîé Health-check")
        btn_pull = gr.Button("‚¨áÔ∏è Baixar modelo agora")

    entrada = gr.Textbox(label="Pergunta sobre o Hacktiba", placeholder="Ex.: Qual a miss√£o do evento?")
    saida = gr.Textbox(label="Resposta", lines=12)
    btn_enviar = gr.Button("Enviar")

    # a√ß√µes
    btn_enviar.click(responder, inputs=entrada, outputs=saida)
    btn_health.click(do_health, inputs=None, outputs=status_md)
    btn_pull.click(do_pull, inputs=None, outputs=saida)

    # faz um health inicial
    init = do_health()[0]
    status_md.value = init

demo.launch(server_name="0.0.0.0", server_port=PORT)
