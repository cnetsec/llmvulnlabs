name: "🧪 Hacktiba 2025 — Lab01 Local"

on:
  workflow_dispatch:
    inputs:
      perguntas:
        description: "Perguntas (JSON array ou separadas por ||)"
        required: true
        type: string
      modelo:
        description: "Escolha o modelo Hugging Face"
        required: true
        type: choice
        options:
          - sshleifer/tiny-gpt2   # super leve (~15MB)
          - distilgpt2            # médio (~500MB)
          - gpt2                  # padrão (~500MB)
      tamanho:
        description: "Quantidade máxima de tokens da resposta"
        required: true
        type: choice
        options:
          - "20"
          - "40"
          - "80"

jobs:
  lab01:
    runs-on: ubuntu-latest
    env:
      HF_HOME: ~/.cache/huggingface
      LAB01_MODEL: ${{ inputs.modelo }}
      LAB01_TAMANHO: ${{ inputs.tamanho }}

    steps:
      - name: "📥 Checkout"
        uses: actions/checkout@v4

      - name: "🐍 Setup Python"
        id: py
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: "📦 Descobrir diretório de cache do pip"
        run: echo "PIP_CACHE_DIR=$(python -m pip cache dir)" >> $GITHUB_ENV

      - name: "🗄️ Cache pip"
        uses: actions/cache@v4
        with:
          path: ${{ env.PIP_CACHE_DIR }}
          key: pip-${{ runner.os }}-${{ steps.py.outputs.python-version }}
          restore-keys: |
            pip-${{ runner.os }}-

      - name: "🗄️ Cache Hugging Face (modelos)"
        uses: actions/cache@v4
        with:
          path: ~/.cache/huggingface
          key: hf-${{ runner.os }}-${{ env.LAB01_MODEL }}-v1
          restore-keys: |
            hf-${{ runner.os }}-

      - name: "📦 Instalar dependências"
        run: |
          python -m pip install --upgrade pip
          pip install --quiet --index-url https://download.pytorch.org/whl/cpu torch
          pip install --quiet transformers

      - name: "🔥 Pré-baixar modelo"
        run: |
          python - <<'PY'
          import os
          from transformers import pipeline
          model = os.environ["LAB01_MODEL"]
          gen = pipeline("text-generation", model=model)
          gen("warmup", max_new_tokens=1)
          print(f"[warm] modelo pronto: {model}")
          PY

      - name: "▶️ Executar Lab01.py"
        env:
          LAB01_PERGUNTAS: ${{ inputs.perguntas }}
          LAB01_MODEL: ${{ env.LAB01_MODEL }}
          LAB01_TAMANHO: ${{ env.LAB01_TAMANHO }}
        run: python Lab01.py
